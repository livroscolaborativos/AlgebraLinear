% Este trabalho está licenciado sob a Licença Creative Commons Atribuição-CompartilhaIgual 3.0 Não Adaptada. Para ver uma cópia desta licença, visite http://creativecommons.org/licenses/by-sa/3.0/ ou envie uma carta para Creative Commons, PO Box 1866, Mountain View, CA 94042, USA.

\documentclass{article}  %%DM%%Escolher document class and options article, etc

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
%%%%%%%% Pacotes básicos para MathEnvir%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

\usepackage{amsmath}   %%AMS primary package (includes amstext, amsopn, amsbsy), provides various features for displayed equations and %%other mathematical constructs.
%%%% OPTIONS FOR THE AMSMATH PACKAGE
\usepackage{amscd}     %%Provides a CD environment for simple commutative diagrams (no support for diagonal arrows).
\usepackage{amsxtra}   %%Provides certain odds and ends such as \fracwithdelims and \accentedsymbol, for compatibility with documents %%created using version 1.1.
\usepackage{amsthm}    %%Enhanced version of \newtheorem command for defining theorem-like environments
\usepackage{amssymb}   %%Provides an extended symbol collection (includes amsfonts). For example, \barwedge, \boxdot, \boxminus, %%\boxplus, \boxtimes, \Cap, \Cup (and many more), the arrow \leadsto, and some other symbols such as \Box and \Diamond.

\usepackage{latexsym}  %%makes few additional characters available: \Box \Join \Box \Diamond \leadsto \sqsubset \sqsupset \lhd \unlhd %%\rhd \unrhd

\usepackage[makeroom]{cancel}   % Cancelar termos em equações

\usepackage{enumerate}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
%%%%%%%% Links -- Só funciona para .pdf%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

% \usepackage{hyperref}  %% for references %%Options below
% \hypersetup{colorlinks=true, linkcolor=blue, citecolor=blue, linktoc=page, pdftitle={Shadowing}, pdfauthor={D. Marcon}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
%%%%%% Color, graphicx, margin (geometry)%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 


\usepackage[dvips]{graphicx}
\usepackage{color}            %%Textcolor, color definitions, etc

% \definecolor{light-blue}{rgb}{0.8,0.85,1}     %%Numbers between 0 and 1
% \definecolor{mygrey}{gray}{0.75}              %%Numbers between 0 and 1

\usepackage{verbatim}         %%Adds text from other files, comment environment

\usepackage{xpatch}           %%Bold theorem titles
\makeatletter
\xpatchcmd{\@thm}{\fontseries\mddefault\upshape}{}{}{} %same font as thm-header
\makeatother

\usepackage[margin=1in]{geometry}  %%Margins  %%Possible to use \newgeometry to modify small parts mid-document



\usepackage{tgbonum}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
%%%%%%%%%%% Para escrever português%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 


% \usepackage[utf8]{inputenc}  %%encoding
% \usepackage[T1]{fontenc}     %%encoding

\usepackage[portuguese]{babel}   %%Portuguese-specific commands



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
%%%%%%%%%% Theorem styles, numbering%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 


\theoremstyle{plain}
\newtheorem{theorem}{Teorema}              %%section, chapter, etc
\newtheorem{proposition}[theorem]{Proposição}      %%everything below obeys theorem because: [theorem]
\newtheorem{lemma}[theorem]{Lema}
\newtheorem{corollary}[theorem]{Corolário}
\newtheorem{maintheorem}{Teorema}                   %%number doesn't obey order
\renewcommand{\themaintheorem}{\Alph{maintheorem}}
\newtheorem{maincorollary}{Corolário}
\newtheorem{conjecture}{Conjectura}
\newtheorem*{claim}{Afirmação}

\newtheorem*{desloct}{Segundo Teorema de Deslocamento -- Deslocamento em $t$}     %%Sem numeração e com o nome desejado

\theoremstyle{definition}
\newtheorem{remark}[theorem]{Observação}
\newtheorem{example}[theorem]{Exemplo}
\newtheorem{definition}[theorem]{Definição}
\newtheorem{exercise}{Exercício}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
%%%%%%%%%%%%%%%% New Commands%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

\newcommand{\mb}[1]{\mathbb{#1}}
\newcommand{\bC}{\mathbb{C}}
\newcommand{\bE}{\mathbb{E}}
\newcommand{\bK}{\mathbb{K}}
\newcommand{\bN}{\mathbb{N}}
\newcommand{\bP}{\mathbb{P}}
\newcommand{\bQ}{\mathbb{Q}}
\newcommand{\bR}{\mathbb{R}}
\newcommand{\bS}{\mathbb{S}}
\newcommand{\bT}{\mathbb{T}}
\newcommand{\bZ}{\mathbb{Z}}

\newcommand{\cB}{\mathcal{B}}
\newcommand{\cE}{\mathcal{E}}
\newcommand{\cF}{\mathcal{F}}
\newcommand{\cH}{\mathcal{H}}
\newcommand{\cL}{\mathcal{L}}
\newcommand{\cM}{\mathcal{M}}
\newcommand{\cO}{\mathcal{O}}
\newcommand{\cP}{\mathcal{P}}
\newcommand{\cQ}{\mathcal{Q}}
\newcommand{\cR}{\mathcal{R}}
\newcommand{\cS}{\mathcal{S}}

\newcommand{\al} {\alpha}       \newcommand{\Al}{\Alpha}
\newcommand{\be} {\beta}        \newcommand{\Be}{\Beta}
\newcommand{\ga} {\gamma}       \newcommand{\Ga}{\Gamma}
\newcommand{\de} {\delta}       \newcommand{\De}{\Delta}
\newcommand{\ep} {\epsilon}
\newcommand{\eps}{\varepsilon}
\newcommand{\ze} {\zeta}
\newcommand{\vte}{\vartheta}
\newcommand{\iot}{\iota}
\newcommand{\ka} {\kappa}
\newcommand{\la} {\lambda}      \newcommand{\La}{\Lambda}
\newcommand{\vpi}{\varpi}
% \newcommand{\ro} {\rho}
\newcommand{\vro}{\varrho}
\newcommand{\si} {\sigma}       \newcommand{\Si}{\Sigma}
\newcommand{\vsi}{\varsigma}
\newcommand{\ups}{\upsilon}     \newcommand{\Up}{\Upsilon}
\newcommand{\vphi}{\varphi}
\newcommand{\om} {\omega}       \newcommand{\Om}{\Omega}

\newcommand{\ang}{\operatorname{angle}}
\newcommand{\closu}{\operatorname{clos}}
\newcommand{\Col}{\operatorname{Col}}
\newcommand{\const}{\operatorname{const}}
\newcommand{\curl}{\operatorname{curl}}
\newcommand{\dd}{\, \mathrm{d}}
\newcommand{\diam}{\operatorname{diam}}
\newcommand{\Dim}{\operatorname{dim}}
\newcommand{\Div}{\operatorname{div}}
\newcommand{\dist}{\operatorname{dist}}
\newcommand{\grad}{\operatorname{grad}}
\newcommand{\fr}{\partial}
\newcommand{\graph}{\operatorname{graph}}
\newcommand{\id}{\operatorname{Id}}
\newcommand{\inter}{\operatorname{int}}
\newcommand{\Leb}{\operatorname{Leb}}
\newcommand{\length}{\operatorname{length}}
\newcommand{\Lip}{\operatorname{Lip}}
\newcommand{\Nul}{\operatorname{Nul}}
\newcommand{\proj}{\operatorname{proj}}
\newcommand{\res}{\operatornamewithlimits{Res}}
\newcommand{\rot}{\operatorname{rot}}
\newcommand{\sen}{\operatorname{sen}}
\newcommand{\senh}{\operatorname{senh}}
\newcommand{\Span}{\operatorname{Span}}
\newcommand{\spec}{\operatorname{spec}}
\newcommand{\supp}{\operatorname{supp}}
\newcommand{\var}{\operatornamewithlimits{{var}}}
\newcommand{\intd}{\operatornamewithlimits{{\iint}}}


\everymath{\displaystyle}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
%%%%%%%%%%%% INFO  DO  ARTIGO %%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

\title{Álgebra Linear -- Semana 13}
\author{}
\date{20 de Junho de 2017}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
%%%%%%%%%%%% INICIO DO DOCUMENTO%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

\begin{document}
\maketitle
\tableofcontents

\vspace{0.5cm}






\section{Método dos Mínimos Quadrados}

Sistemas lineares aparecem como modelos matemáticos de vários fenômenos e em várias situações. Acontece que alguns sistemas simplesmente não possuem soluções e ficamos sem saber como proceder. O \textbf{Método dos Mínimos Quadrados} é uma técnica que nos permite, de forma aproximada, retirar alguma informação desses sistemas impossíveis. A terminologia se deve ao fato de que, como veremos, este método minimiza a soma dos quadrados dos erros obtidos na aproximação.

Como sabemos, resolver o sistema linear
\[
A \vec{x} = \vec{b}
\] consiste em encontrar um vetor $\vec{x}$ que satisfaça a esta equação. Na terminologia que construimos ao longo do curso, isto significa que $\vec{b}$ pertence ao espaço coluna da matriz $A$, isto é, $\vec{b} \in \Col A$. Desta forma, não é possível resolver o sistema quando $\vec{b} \not\in \Col A$. 
\begin{figure}[h!]
  \begin{center}
    \includegraphics[width=0.6\linewidth]{semana13-proj-col.png}
  \end{center}
\end{figure}

O Método dos Mínimos Quadrados consiste em olhar para o vetor $\vec{p} = \proj_{\Col A} \vec{b}$ e resolver o sistema linear associado
\[
A \vec{x} = \vec{p}.
\] Esta solução de $A \vec{x} = \vec{p}$ é chamada de \textbf{solução de mínimos quadrados}. A ideia é (ver figura) considerar o vetor em $\Col A$ que é ``mais próximo'' de $\vec{b}$ e cujo sistema linear associado possua solução. Neste contexto, estamos pensando em mais próximo no sentido que
\[
\| \vec{b} - \proj_{\Col A} \vec{b} \| \le \| \vec{b} - \vec{c} \|, \text{ para qualquer } \vec{c}\, \in \Col A,
\] isto é, no sentido de ser a projeção a melhor aproximação de $\vec{b}$ no espaço coluna de $A$. Escrevendo os vetores em coordenadas:
\[
\vec{b} =
\begin{bmatrix}
  b_1 \\ b_2 \\ \vdots \\ b_n
\end{bmatrix} \ \text{e} \
\proj_{\Col A} \vec{b} =
\begin{bmatrix}
  p_1 \\ p_2 \\ \vdots \\ p_n
\end{bmatrix},
\] temos que o valor
\[
\| \vec{b} - \proj_{\Col A} \vec{b} \| = \sum_{i=1}^n (b_i - p_i)^2
\] é chamado de \textbf{erro da aproximação}. Logo, como anunciado, a soma dos quadrados dos erros obtidos cada componente é o mínimo possível.


\begin{example}\label{exp:minquad1}
  Considere o sistema linear
  \[
  \left\{
    \begin{array}{ll}
      x_1 + x_2 - 2x_3 = 3 \\
      x_1  - 2x_3 = 2 \\
      x_2 + 2x_3 = 0 \\
      x_1 + x_2 - 2x_3 = 0
    \end{array}
  \right. \ \leftrightsquigarrow  \
  A \vec{x} = \begin{bmatrix}
    1 & 1 & -2 \\ 1 & 0 & -2 \\ 0 & 1 & 2 \\ -1 & -1 & 2
  \end{bmatrix}
  \begin{bmatrix}
    x_1 \\ x_2 \\ x_3
  \end{bmatrix} =
  \begin{bmatrix}
    3 \\ 2 \\ 0 \\ 0
  \end{bmatrix} = \vec{b}.
  \] Este sistema é claramente impossível, pois a primeira equação é inconsistente com a última. De forma mais automática, um passo no escalonamento da matriz aumentada associada revelaria a mesma conclusão:
  \[
  [\, A \ | \ \vec{b} \, ] = \begin{bmatrix}
    1 & 1 & -2 & 3 \\ 1 & 0 & -2 & 2 \\ 0 & 1 & 2 & 0 \\ -1 & -1 & 2 & 0
  \end{bmatrix}   \sim
  \begin{bmatrix}
    1 & 1 & -2 & 3 \\ 1 & 0 & -2 & 2 \\ 0 & 1 & 2 & 0 \\ 0 & 0 & 0 & 3
  \end{bmatrix} .
  \] Vamos tentar encontrar, conforme descrito acima, uma solução de mínimos quadrados. Passos a serem realizados:
  \begin{itemize}
  \item Para calcular a projeção de $\vec{b}$ sobre $\Col A$, precisamos, em primeiro lugar, obter uma base ortogonal do espaço $\Col A$; isto pode ser feito pelo Processo de Gram--Schmidt;
  \item Calcular a projeção $\vec{p} = \proj_{\Col A} \vec{b}$;
  \item Resolver o sistema linear $A \vec{x} = \vec{p}$.
  \end{itemize} Embora seja possível realizar estes três passos, temos de convir que seria muito trabalhoso. Por isto, vamos analisar um pouco mais da teoria para encontrar um método mais eficaz$. \ \lhd$
\end{example}

Suponhamos que:
\begin{itemize}
\item $A$ é uma matriz $m \times n$,
\item $\vec{x} \in \bR^n$,
\item $\vec{b} \in \bR^m$ e
\item o sistema $A \vec{x} = \vec{b}$ não possui solução.
\end{itemize} A solução de mínimos quadrados é a solução de $A \vec{x} = \vec{p}$, onde $\vec{p} = \proj_{\Col A} \vec{b}$. Observamos, por outro lado, que, sendo $\vec{p}\,$ a projeção sobre o espaço coluna de $A$, o vetor $\vec{b} - \vec{p}\,$ deve ser ortogonal a todos os elementos de $\Col A$. Em particular, se escrevermos $A$ em termos de suas colunas
\[
A =
\begin{bmatrix}
  | & | &   & | \\
  \vec{a}_1 & \vec{a}_2 & \cdots  & \vec{a}_n \\
  | & | &   & |
\end{bmatrix}, \qquad \left( \text{que,  em particular, implica }
  A^T = \begin{bmatrix}
    \text{---} & \vec{a}_1  & \text{---} \\
    \text{---} & \vec{a}_2  & \text{---} \\
    & \vdots     &  \\
    \text{---} & \vec{a}_n  & \text{---}
  \end{bmatrix}\right)
\] devemos ter
\[
\vec{a}_j \cdot \big(\vec{b} - \vec{p}\big) = 0, \text{ para todo } j  \ \  \iff \ \
A^T (\vec{b} - \vec{p}) = \vec{0} \ \ \iff \ \ A^T \vec{b} = A^T \vec{p} = A^T A\vec{x}.
\] Esta última linha é válida porque o produto escalar $\vec{a}_j \cdot \big(\vec{b} - \vec{p}\big)$ é justamente a entrada $j$ do vetor obtido ao multiplicar $A^T(\vec{b} - \vec{p})$.

Concluimos, desta forma, que, se $\vec{x}$ é uma solução de mínimos quadrados, ou seja, se $A \vec{x} = \vec{p}$, então necessariamente
\begin{equation}\label{minquad}
  \boxed{A^T A\vec{x} = A^T \vec{b}.}
\end{equation}

\begin{exercise}[Teórico]
  Justifique que o conjunto de soluções do sistema \eqref{minquad} coincide com o conjunto das soluções de mínimos quadrados do sistema $A \vec{x} = \vec{b}$.
\end{exercise}

Tudo isto implica que podemos utilizar a equação \eqref{minquad} para encontrar as soluções de mínimos quadrados de $A \vec{x} = \vec{b}$. 

\begin{example}[De volta ao Exemplo \ref{exp:minquad1}]\label{exp:minquad2}
  Calculamos
  \[
  A^T A =
  \begin{bmatrix}
    1 & 1 & 0 & -1 \\ 1 & 0 & 1 & -1 \\ -2 & -2 & 2 & 2
  \end{bmatrix}
  \begin{bmatrix}
    1 & 1 & -2 \\ 1 & 0 & -2 \\ 0 & 1 & 2 \\ -1 & -1 & 2
  \end{bmatrix} =
  \begin{bmatrix}
    3 & 2 & -6 \\ 2 & 3 & -2 \\ -6 & -2 & 16
  \end{bmatrix}
  \] e
  \[
  A^T \vec{b} =
  \begin{bmatrix}
    1 & 1 & 0 & -1 \\ 1 & 0 & 1 & -1 \\ -2 & -2 & 2 & 2
  \end{bmatrix}
  \begin{bmatrix}
    3 \\ 2 \\ 0 \\ 0
  \end{bmatrix} =
  \begin{bmatrix}
    5 \\ 3 \\ -10
  \end{bmatrix}
  \] Para resolvermos o sistema $A^TA\vec{x} = A^T\vec{b}$, vamos escalonar a matriz aumentada associada:
  \[
  \begin{bmatrix}
    3 & 2 & -6 & 5 \\
    2 & 3 & -2 & 3 \\
    -6 & -2 & 16 & -10
  \end{bmatrix} \sim
  \begin{bmatrix}
    3 & 2 & -6 & 5 \\
    0 & 5 &  6 & -1 \\
    0 & 2 &  4 & 0
  \end{bmatrix} \sim
  \begin{bmatrix}
    3 & 2 & -6 & 5 \\
    0 & 5 &  6 & -1 \\
    0 & 0 &  4 & 1
  \end{bmatrix}
  \] Logo,
  \[
  \left\{
    \begin{array}{ll}
      3 x_1 + 2 x_2 - 6 x_3 = 5 \\
      5x_2 + 6 x_3 = -1 \\
      4x_3 = 1
    \end{array}
  \right..
  \] Assim,
  \[
  x_3 = \frac{1}{4} \implies 5x_2 + 6 \cdot \frac{1}{4} = -1 \implies x_2 = -\frac{1}{2} \implies 3 x_1 - 1 - \frac{3}{2}  = 5 \implies x_1 = \frac{3}{2}.
  \] Portanto, uma solução de mínimos quadrados é
  \[
  \begin{bmatrix}
    x_1 \\ x_2 \\ x_3
  \end{bmatrix} =
  \begin{bmatrix}
    3/2 \\ -1/2 \\ 1/4
  \end{bmatrix}.
  \] Observe como isto é muito menos trabalhoso do que o método que havíamos esquematizado quando discutimos este sistema no Exemplo \ref{exp:minquad1}$. \ \lhd$
\end{example}

\begin{exercise}
  Colocar em prática o método discutido no Exemplo \ref{exp:minquad1} e comparar o resultado com o que obtivemos no Exemplo \ref{exp:minquad2}.
\end{exercise}

\begin{remark}
  Caso a matriz $A$ já seja uma matriz ortogonal, podemos calcular projeções diretamente, pois uma base ortogonal para $\Col A$ já esté disponível desde o começo. Neste caso, ambos os métodos exigiriam um trabalho parecido.
\end{remark}

Nossa aproximação linear foi encontrada de modo a minimizar o erro quadrático. Para encontrar o erro, deveríamos calcular
\[
\| \vec{b} - \proj_{\vec{\Col A}} \vec{b} \|.
\] No entanto, resolvemos de outra maneira e não calculamos a projeção. Observando que a solução de mínimos quadrados satisfaz $A \vec{x} = \proj_{\vec{\Col A}} \vec{b}$, podemos calcular o erro por
\[
\text{erro } = \| \vec{b} - A \vec{x} \|,
\] onde $\vec{x}$ é a solução de mínimos quadrados.

No Exemplo \ref{exp:minquad2} acima, podemos calcular o erro desta forma. De fato,
\[
A \vec{x} = 
\begin{bmatrix}
  1 & 1 & -2 \\ 
  1 & 0 & -2 \\ 
  0 & 1 &  2 \\ 
  -1 & -1&  2
\end{bmatrix}
\begin{bmatrix}
  3/2 \\ -1/2 \\ 1/4
\end{bmatrix} = 
\begin{bmatrix}
  1/2 \\ 1 \\ 0 \\ -1/2
\end{bmatrix} \implies 
\vec{b} - A\vec{x} = 
\begin{bmatrix}
  5/2 \\ 1 \\ 0 \\ 1/2
\end{bmatrix}
\]
\[
\implies \text{erro } = \| \vec{b} - A \vec{x} \| = \sqrt{\frac{25}{4} + 1 + \frac{1}{4}} = \frac{\sqrt{30}}{2}.
\]


% \newpage

\section{Fatoração QR e Mínimos Quadrados}

Vimos que o processo de Gram-Schmidt nos permite escrever uma matriz $A$ cujas linhas são linearmente independentes na forma $A=QR$, onde Q é uma matriz ortogonal, $R$ é triangular superior invertível, e as colunas de $Q$ formam uma base ortonormal para o espaço-coluna de $A$. Chamamos esta fatoração de $A$ de fatoração QR. 

A fatoração QR tem uma importante aplicação na solução de problemas em mínimos quadrados. Ocorre que, em muitos problemas, a matriz $A^T A$ é uma matriz {\it mal-condicionada}, ou seja uma matriz que é muito sensível aos erros de aproximação cometidos em cálculos numéricos usualmente executados no tratamento de problemas aplicados. 
Os exemplos que virão a seguir nos mostram situações em que há uma grande variação de magnitude entre as entradas da matriz $A^T A$, e isto costuma ser fonte de instabilidade numérica.

Para isso, a fatoração $QR$ oferece uma solução alternativa em mínimos quadrados para um sistema linear $Ax=b$:
Se $A=QR$, então 
$$
A^T=R^T Q^T$$
% \;\; \mbox{e}
e
$$  A^T A= R^T Q^T Q R= R^T R,
$$ 
onde usamos o fato de que $Q^{-1}=Q^T$.
Assim as equações normais $A^T A \vec{x} = A^T \vec{b}$ se reduzem a 
$$ R^T R \vec{x} = R^T Q^T \vec{b}.$$
Ainda, como $R$ é invertível podemos eliminar (cancelar) $R^T$ na equação acima, %ficando com a seguinte expressão para a solução em mínimos quadrados
obtendo o seguinte:
\begin{theorem}
  Se $A=QR$ com $Q$ ortogonal e $R$ triangular superior invertível, então 
  a solução em mínimos quadrados do sistema $Ax=b$ é única e dada pela solução do sistema
  $$  R \vec{x} =  Q^T \vec{b}.$$
\end{theorem}

Note que o fato de $R$ ser triangular superior torna a resolução do sistema acima pouco custosa do ponto de vista computacional.

Alternativamente temos uma expressão explícita para solução em mínimos quadrados: $$   \vec{x} = R^{-1} Q^T \vec{b}.$$

É importante notar que existem algoritmos para fatoração QR que são \it{numéricamente estáveis}, ou seja pouco suscetiveis a erros de aproximação ou arredondamento. Tais algorimos são diferentes do processo de Gram Schmidt usual.


\section{Regressão Linear Simples}

Vamos apresentar uma aplicação de algumas das técnicas da seção anterior à Estatística ou Econometria. Uma \textbf{regressão linear simples} é uma equação, tipicamente da forma
\[
y = a + b x,
\] para estimar os valores $(x,y)$ apenas conhecendo alguns valores específicos $(x_1, y_1), (x_2, y_2), \dots,$ $(x_k, y_k)$. A ideia é tentar capturar como que mudanças na variável independente $x$ afetam a variável dependente $y$.

\begin{example}\footnote{Exemplo adaptado de https://onlinecourses.science.psu.edu/stat501/node/257}\label{exp:idade}
  Com o intuito de analisar se é razoável supor que há um relação linear entre a idade de um motorista e quão longe ele consegue ver, uma empresa (Last Resource, Inc., Bellefonte, PA) coletou dados de 30 motoristas. Para simplificar as nossas contas, vamos listar abaixo \textit{apenas alguns} destes dados.
  \begin{center}
    \begin{tabular}{|c|c|}
      \hline
      % after \\: \hline or \cline{col1-col2} \cline{col3-col4} ...
      Idade & Distância (em $m$) \\ \hline
      20 & 590 \\
      32 & 410 \\
      41 & 460 \\
      49 & 380 \\
      66 & 350 \\
      \hline
    \end{tabular}
  \end{center} Podemos pensar em $y$ como a distância e em $x$ como a idade. Gostaríamos de achar uma relacção linear da forma
  \[
  y = a + b x.
  \] Desta forma, os dados obtidos implicam que  
  \[
  \left\{
    \begin{array}{ll}
      b + 20 a = 590 \\
      b + 32 a = 410 \\
      b + 41 a = 460 \\
      b + 49 a = 380 \\
      b + 66 a = 350
    \end{array}
  \right. \ \ \leftrightsquigarrow \ \
  \begin{bmatrix}
    1 & 20 \\
    1 & 32 \\
    1 & 41 \\
    1 & 49 \\
    1 & 66 \\
  \end{bmatrix}
  \begin{bmatrix}
    a \\ b
  \end{bmatrix} =
  \begin{bmatrix}
    590 \\ 410 \\ 460 \\ 380 \\ 350
  \end{bmatrix}
  \] Ora, dificilmente um sistema linear com duas incógnitas e cinco equações terá solução (só terá solução se todos os pontos do conjunto de dados estiverem alinhados em uma reta perfeita!). Vamos procurar por uma solução de mínimos quadrados. Isto é \textbf{regressão linear simpes}.

  Denotando
  \[
  A =
  \begin{bmatrix}
    1 & 20 \\
    1 & 32 \\
    1 & 41 \\
    1 & 49 \\
    1 & 66 \\
  \end{bmatrix} \quad \text{e} \quad
  \vec{b} =
  \begin{bmatrix}
    590 \\ 410 \\ 460 \\ 380 \\ 350
  \end{bmatrix}
  \] precisamos calcular
  \[
  A^T A =
  \begin{bmatrix}
    1 & 1 & 1 & 1 & 1 \\
    20 & 32 & 41 & 49 & 66
  \end{bmatrix}
  \begin{bmatrix}
    1 & 20 \\
    1 & 32 \\
    1 & 41 \\
    1 & 49 \\
    1 & 66 \\
  \end{bmatrix} =
  \begin{bmatrix}
    5    & 208 \\
    208  & 9832  \\
  \end{bmatrix}
  \] e
  \[
  A^T \vec{b} =
  \begin{bmatrix}
    1 & 1 & 1 & 1 & 1 \\
    20 & 32 & 41 & 49 & 66
  \end{bmatrix}
  \begin{bmatrix}
    590 \\ 410 \\ 460 \\ 380 \\ 350
  \end{bmatrix} =
  \begin{bmatrix}
    2190 \\ 85500
  \end{bmatrix}.
  \] Em seguida, a matriz associada aumentada pode ser reduzida por escalonamento:
  \[
  [\, A^TA \ | \ \vec{b} \, ] =
  \begin{bmatrix}
    5    & 208 & 2190 \\
    208  & 9832 & 85500 \\
  \end{bmatrix} \sim
  \begin{bmatrix}
    1 & 0 & 468510/737 \\
    0 & 1 & -7005/1474 \\
  \end{bmatrix} \implies
  \left\{
    \begin{array}{ll}
      a \simeq 635.7 \\
      b \simeq -4.75
    \end{array}
  \right..
  \] Os números são feios, mas as contas feitas foram as que sempre fizemos ao escalonar uma matriz até sua forma escalonada reduzida.

  A conclusão é que a reta de mínimos quadrados que melhor aproxima os nossos dados é a reta
  \[
  y = a + b x = 635.7 - 4.75 x.
  \] O erro de mínimos quadrados nesta aproximação (ou norma do resíduo) pode ser calculado como
  \[
  A \vec{x} = \begin{bmatrix}
    1 & 20 \\
    1 & 32 \\
    1 & 41 \\
    1 & 49 \\
    1 & 66 \\
  \end{bmatrix}
  \begin{bmatrix}
    635.7 \\
    4.75 \\
  \end{bmatrix} \simeq
  \begin{bmatrix}
    730.7 \\
    787.7 \\
    830.45 \\
    868.45 \\
    949.2 \\
  \end{bmatrix} \simeq \proj_{\Col A} \vec{b} \implies \text{ erro } = \| \vec{b} - A\vec{x} \| \simeq 947.3
  \] Na figura abaixo, mostramos um esboço da reta que melhor aproxima os dados deste exemplo$. \ \lhd$
  \begin{figure}[h!]
    \begin{center}
      \includegraphics[width=1\linewidth]{semana13-idade.png}
    \end{center}
  \end{figure}
\end{example}



De uma maneira geral, para encontrar uma \textbf{reta de melhor ajuste} a uma quantidade de pontos (dados coletados de algum problema)
\[
(x_1, y_1), \ (x_2, y_2), \ \dots, (x_k, y_k),
\] devemos procurar pela solução $(a,b)$ de mínimos quadrados do sistema linear
\[
\left\{
  \begin{array}{rl}
    a + b x_1 &\!\!\!\!\!= y_1  \\
    a + b x_2 &\!\!\!\!\!= y_2  \\
    \vdots &  \\
    a + b x_k &\!\!\!\!\!= y_k  \\
  \end{array}
\right. \quad \leftrightsquigarrow  \quad
\begin{bmatrix}
  1 & x_1 \\
  1 & x_2 \\
  \vdots & \vdots \\
  1 & x_k \\
\end{bmatrix}
\begin{bmatrix}
  a \\ b
\end{bmatrix} =
\begin{bmatrix}
  y_1 \\ y_2 \\ \vdots \\ y_k
\end{bmatrix}.
\] Vejamos um outro exemplo:

\begin{example}
  Encontrar a reta que melhor se ajusta aos pontos do plano
  \[
  (1,2), \ \ (2,3), \ \ (2,4), \ \ (3,2), \ \ (4,3), \ \ (4,4), \ \ (5,3), \ \ (5,5), \ \ (6,4).
  \] Como vimos acima, podemos considerar
  \[
  A =
  \begin{bmatrix}
    1 & 1 \\
    1 & 2 \\
    1 & 2 \\
    1 & 3 \\
    1 & 4 \\
    1 & 4 \\
    1 & 5 \\
    1 & 5 \\
    1 & 6 \\
  \end{bmatrix} \ \ \text{e} \ \
  \vec{b} =
  \begin{bmatrix}
    2\\3\\4\\2\\3\\4\\3\\5\\4
  \end{bmatrix}
  \] Assim,
  \[
  A^T A =
  \begin{bmatrix}
    9  & 32  \\
    32  & 136
  \end{bmatrix}  \ \ \text{e} \ \
  A^T \vec{b} =
  \begin{bmatrix}
    30\\114
  \end{bmatrix}.
  \] A solução de mínimos quadrados, que é a solução de $A^T A \vec{x} = A^T\vec{b}$, é dada por
  \[
  b = 54/25 = 2.16, \\ a = 33/100 = 0.33.
  \] A reta procurada é portanto $y = 0.33 x + 2.16$.
  \begin{figure}[h!]
    \begin{center}
      \includegraphics[width=0.5\linewidth]{semana13-reta.png}
    \end{center}
  \end{figure}
\end{example}


\section{Regressão não linear}


Podemos também procurar (em contraste com a seção anterior) por funções não lineares que se ajustem a um conjunto de pontos. Por exemplo, dado um certo conjunto de pontos
\[
(x_1, y_1), \ (x_2, y_2), \ \dots, (x_k, y_k),
\] vamos procurar por uma parábola
\[
y = a + bx + cx^2
\] que melhor se ajuste a este conjunto de pontos, no sentido de que a soma dos quadrados dos erros seja a menor possível. Isto corrensponde a procurar pela solução $(a,b, c)$ de mínimos quadrados do seguinte sistema linear
\[
\left\{
  \begin{array}{rl}
    a + b x_1 + c x_1^2 &\!\!\!\!\!= y_1  \\
    a + b x_2 + c x_2^2 &\!\!\!\!\!= y_2  \\
    \vdots &  \\
    a + b x_k + c x_k^2 &\!\!\!\!\!= y_k  \\
  \end{array}
\right. \quad \leftrightsquigarrow  \quad
\begin{bmatrix}
  1 & x_1 & x_1^2 \\
  1 & x_2 & x_2^2 \\
  \vdots & \vdots \\
  1 & x_k & x_k^2 \\
\end{bmatrix}
\begin{bmatrix}
  a \\ b \\ c
\end{bmatrix} =
\begin{bmatrix}
  y_1 \\ y_2 \\ \vdots \\ y_k
\end{bmatrix}.
\] Vejamos como ficaria a parábola que se ajusta aos dados do Exemplo \ref{exp:idade}:

\begin{example}
  Nossos pontos no Exemplo \ref{exp:idade} são
  \[
  (20, 590), \ (32, 410), \ (41, 460), \ (49, 380), \ (66, 350).
  \] Para encontrar a parábola de melhor ajuste como acima, devemos procurar pela solução de mínimos quadrados do sistema
  \[
  \begin{bmatrix}
    1 & 20 & 400 \\
    1 & 32 & 1024 \\
    1 & 41 & 1681 \\
    1 & 49 & 2401 \\
    1 & 66 & 4356 \\
  \end{bmatrix}
  \begin{bmatrix}
    a \\ b \\ c
  \end{bmatrix} =
  \begin{bmatrix}
    590 \\ 410 \\ 460 \\ 380 \\ 350
  \end{bmatrix} \ \leftrightsquigarrow \ A \vec{x} = \vec{b}.
  \] Calculamos (com o auxílio de uma calculadora)
  \[
  A^T A =
  \begin{bmatrix}
    1 & 1 & 1 & 1 & 1 \\
    20 & 32 & 41 & 49 & 66 \\
    400 & 1024 & 1681 & 2401 & 4356 \\
  \end{bmatrix}
  \begin{bmatrix}
    1 & 20 & 400 \\
    1 & 32 & 1024 \\
    1 & 41 & 1681 \\
    1 & 49 & 2401 \\
    1 & 66 & 4356 \\
  \end{bmatrix} =
  \begin{bmatrix}
    5 & 208 & 9862 \\
    208 & 9862 & 514834 \\
    9862 & 514834 & 28773874 \\
  \end{bmatrix},
  \]
  \[
  A^T A =
  \begin{bmatrix}
    1 & 1 & 1 & 1 & 1 \\
    20 & 32 & 41 & 49 & 66 \\
    400 & 1024 & 1681 & 2401 & 4356 \\
  \end{bmatrix}
  \begin{bmatrix}
    590 \\ 410 \\ 460 \\ 380 \\ 350
  \end{bmatrix} =
  \begin{bmatrix}
    2190 \\ 85500 \\ 3866080
  \end{bmatrix}
  \] e resolvemos (escalonando, por exemplo, com o auxílio de um computador)
  \[
  \begin{bmatrix}
    5 & 208 & 9862 & 2190 \\
    208 & 9862 & 514834 & 85500 \\
    9862 & 514834 & 28773874 & 3866080 \\
  \end{bmatrix} \sim
  \begin{bmatrix}
    1 & 0 & 0 & 28482529840/35036713 \\
    0 & 1 & 0 & -1505841055/105110139 \\
    0 & 0 & 1 & 11779375/105110139 \\
  \end{bmatrix}.
  \] Logo, aproximando estas frações, obtemos
  \[
  \left\{
    \begin{array}{ll}
      a \simeq 812.934 \\
      b \simeq -14.326 \\
      c \simeq  0.112 \\
    \end{array}
  \right.
  \] e a parábola desejada é, aproximadamente
  \[
  y = 812.934 - 14.326 x + 0.112 x^2.
  \]
  \begin{figure}[h!]
    \begin{center}
      \includegraphics[width=1\linewidth]{semana13-idade-parabola.png}
    \end{center}
  \end{figure}

  \noindent Observamos que, apesar de o erro quadrático ser provavelmente menor, esta parábola se torna crescente a partir de um certo momento, fazendo com que o modelo não seja tão razoável para idades maiores. Por exemplo, é pouco provável que (na média), pessoas com 95 anos vejam a uma distância maior do que pessoas de 65 anos, como sugere a parábola acima.
\end{example}



\section{Regressão linear múltipla}

Uma \textbf{regressão linear múltipla} é o problema análogo à regressão linear simples no caso em que a variável dependente pode depender de mais fatores independentes. Tipicamente, queremos encontrar uma equação afim que melhor se ajusta a alguns dados conhecidos.

No caso especial em que $y$ depende de apenas outros dois fatores, escreveremos
\[
z = a + b x + c y,
\] que, geometricamente, representa um plano no espaço tridimensional $\bR^3$. Seja agora uma conjunto de dados:
\[
(x_1, y_1, z_1), \ (x_1, y_1, z_1), \dots, (x_k, y_k, z_k).
\] Queremos encontrar coeficientes $(a,b,c)$ que satisfaçam:
\[
\left\{
  \begin{array}{c}
    a + b x_1 + c y_1 = z_1 \\
    a + b x_2 + c y_2 = z_2 \\
    \vdots \\
    a + b x_k + c y_k = z_k \\
  \end{array}
\right. \quad \leftrightsquigarrow \quad
\begin{bmatrix}
  1 & x_1 & y_1 \\
  1 & x_2 & y_2 \\
  \vdots & \vdots & \vdots \\
  1 & x_k & y_k \\
\end{bmatrix}
\begin{bmatrix}
  a \\ b \\ c
\end{bmatrix} =
\begin{bmatrix}
  z_1 \\ z_2 \\ \vdots \\ z_k
\end{bmatrix}.
\] Quanto maior o número de dados, mais provável de o sistema ser impossível (podemos fazer a analogia geométrica de que por três pontos não colineares no espaço passa um único plano; se aumentarmos o número de pontos, mais difícil que haja um plano contendo todos eles). Por isto, procuramos por uma solução de mínimos quadrados.



\begin{example}
  Uma pesquisa com $214$ mulheres na em uma universidade americana\footnote{Referência: \verb"https://onlinecourses.science.psu.edu/stat501/node/292"} coletou informações sobre a altura das participantes, assim como a altura de seus pais. Abaixo, listamos \textit{apenas alguns destes dados}, para que nossas contas não fiquem tão extensas. Fizemos também uma mudança de unidades nas alturas (de polegadas) para centímetros,
  \begin{center}
    \begin{tabular}{|c|c|c|}
      \hline
      % after \\: \hline or \cline{col1-col2} \cline{col3-col4} ...
      Altura & Altura Mãe & Altura Pai \\ \hline
      152 & 155 & 165 \\
      162 & 155 & 160 \\
      165 & 170 & 173 \\
      170 & 163 & 183 \\
      173 & 168 & 183 \\
      183 & 165 & 183 \\
      \hline
    \end{tabular}
  \end{center} Queremos encontrar uma solução de mínimos quadrados para o sistema linear
  \[
  \begin{bmatrix}
    1 & 155 & 165 \\
    1 & 155 & 160 \\
    1 & 170 & 173 \\
    1 & 163 & 183 \\
    1 & 168 & 183 \\
    1 & 165 & 183 \\
  \end{bmatrix}
  \begin{bmatrix}
    a \\ b \\ c
  \end{bmatrix} =
  \begin{bmatrix}
    152  \\
    162  \\
    165  \\
    170  \\
    173  \\
    183  \\
  \end{bmatrix} \ \leftrightsquigarrow \ A \vec{x} = \vec{b}.
  \] Calculamos
  \[
  A^TA =
  \begin{bmatrix}
    1 & 1 & 1 & 1 & 1 & 1 \\
    155 & 155 & 170 & 163 & 168 & 165 \\
    165 & 160 & 173 & 183 & 183 & 183 \\
  \end{bmatrix}
  \begin{bmatrix}
    1 & 155 & 165 \\
    1 & 155 & 160 \\
    1 & 170 & 173 \\
    1 & 163 & 183 \\
    1 & 168 & 183 \\
    1 & 165 & 183 \\
  \end{bmatrix} =
  \begin{bmatrix}
    6 & 976 & 1047 \\
    976 & 158968 & 170553 \\
    1047 & 170553 & 183221 \\
  \end{bmatrix}
  \] e
  \[
  A^T \vec{b} =
  \begin{bmatrix}
    1 & 1 & 1 & 1 & 1 & 1 \\
    155 & 155 & 170 & 163 & 168 & 165 \\
    165 & 160 & 173 & 183 & 183 & 183 \\
  \end{bmatrix}
  \begin{bmatrix}
    152  \\
    162  \\
    165  \\
    170  \\
    173  \\
    183  \\
  \end{bmatrix} =
  \begin{bmatrix}
    1005  \\
    163689  \\
    175803 \\
  \end{bmatrix}.
  \] Por escalonamento,
  \[
  \begin{bmatrix}
    6    & 976    & 1047   & 1005     \\
    976  & 158968 & 170553 & 163689   \\
    1047 & 170553 & 183221 & 175803   \\
  \end{bmatrix} \sim
  \begin{bmatrix}
    1 & 0 & 0  & 2154573/145769 \\
    0 & 1 & 0  & 14475/145769   \\
    0 & 0 & 1  & 114081/145769  \\
  \end{bmatrix} \implies
  \left\{
    \begin{array}{ll}
      a \simeq 14.781  \\
      b \simeq  0.099  \\
      c \simeq  0.783  \\
    \end{array}
  \right.
  \] A equação de melhor ajuste procurada é, portanto, aproximadamente,
  \[
  z \simeq 14.781 + 0.099 x + 0.783 y.
  \] Tente calcular sua altura $z$ a partir da altura de sua mãe $x$ e de seu pai $y$. O teste deve funcionar melhor para mulheres!
  \begin{figure}[h!]
    \begin{center}
      \includegraphics[width=1\linewidth]{semana13-alturas.png}
    \end{center}
  \end{figure}
\end{example}




\end{document} 